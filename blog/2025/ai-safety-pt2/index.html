<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the Safety Theater - What Real AI Safety Looks Like (Part 2) | Danial Amin </title> <meta name="author" content="Danial Amin"> <meta name="description" content="With AI companies collectively failing basic safety standards while racing toward AGI, we need radical reforms that go far beyond voluntary pledges and self-assessment. Here's what genuine AI safety accountability would require—and why the industry won't adopt it voluntarily."> <meta name="keywords" content="human-computer interaction, generative ai, personas, user representation, fairness"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?05bb9ffd5c923af1a6eccf0d57836de3"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://damin-acad.github.io/blog/2025/ai-safety-pt2/"> <script src="/assets/js/theme.js?cef5f310457b5b065775c7a31be91f90"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.solution-box{background:linear-gradient(135deg,#28a745 0%,#1e7e34 100%);color:white;padding:1.5rem;border-radius:.5rem;margin:2rem 0}.action-box{background:#e7f3ff;border-left:4px solid #06c;padding:1rem;margin:2rem 0;border-radius:.25rem}.reality-box{background:#f8f9fa;border-left:4px solid #6c757d;padding:1rem;margin:2rem 0;border-radius:.25rem}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Beyond the Safety Theater - What Real AI Safety Looks Like (Part 2)",
            "description": "With AI companies collectively failing basic safety standards while racing toward AGI, we need radical reforms that go far beyond voluntary pledges and self-assessment. Here's what genuine AI safety accountability would require—and why the industry won't adopt it voluntarily.",
            "published": "July 26, 2025",
            "authors": [
              
              {
                "author": "Danial Amin",
                "authorURL": "https://linkedin.com/in/danial-amin",
                "affiliations": [
                  {
                    "name": "Samsung Design Innovation Center, France",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Danial</span> Amin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Beyond the Safety Theater - What Real AI Safety Looks Like (Part 2)</h1> <p>With AI companies collectively failing basic safety standards while racing toward AGI, we need radical reforms that go far beyond voluntary pledges and self-assessment. Here's what genuine AI safety accountability would require—and why the industry won't adopt it voluntarily.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-end-of-self-regulation">The End of Self-Regulation</a> </div> <div> <a href="#mandatory-third-party-oversight">Mandatory Third-Party Oversight</a> </div> <div> <a href="#whistleblowing-that-actually-works">Whistleblowing That Actually Works</a> </div> <div> <a href="#real-existential-safety-planning">Real Existential Safety Planning</a> </div> <div> <a href="#global-coordination-beyond-culture-wars">Global Coordination Beyond Culture Wars</a> </div> <div> <a href="#the-economic-reality-check">The Economic Reality Check</a> </div> <div> <a href="#what-success-would-look-like">What Success Would Look Like</a> </div> </nav> </d-contents> <p>The diagnosis is clear from Part 1: the AI industry is systemically failing at safety while racing toward potentially catastrophic capabilities. But identifying the disease is only half the battle. The harder question is what genuine AI safety accountability would actually look like—and why the industry’s current trajectory makes voluntary reform impossible.</p> <p><strong>The brutal truth is that market incentives alone cannot solve AI safety.</strong> When companies are competing to build AGI first in a winner-take-all market, safety measures that slow development become competitive disadvantages. The only solution is external constraint that applies equally to all players.</p> <div class="solution-box"> <strong>The Core Principle:</strong> Real AI safety requires independent oversight with enforcement power, mandatory transparency standards, and accountability mechanisms that can't be gamed by the companies themselves. Anything less is just safety theater with apocalyptic stakes. </div> <h2 id="the-end-of-self-regulation">The End of Self-Regulation</h2> <p>The AI Safety Index results prove that self-regulation has failed catastrophically. Companies claiming to build AGI within the decade can’t even achieve basic competency in safety planning, yet we’re supposed to trust them to voluntarily constrain themselves when billions of dollars and civilizational influence hang in the balance.</p> <p><strong>This is not a market failure—it’s a predictable result of misaligned incentives.</strong></p> <h3 id="why-voluntary-standards-dont-work">Why Voluntary Standards Don’t Work</h3> <p>The index reveals three fatal flaws in the self-regulatory approach:</p> <p><strong>1. No Common Floor</strong>: With no mandatory standards, companies adopt dramatically different safety practices. While Anthropic conducts human bio-risk trials, DeepSeek addresses “extreme jailbreak vulnerability” as an afterthought. This fragmentation creates a race to the bottom.</p> <p><strong>2. No External Verification</strong>: Even companies that claim to implement safety measures control their own evaluation and disclosure. The index found that methodologies linking evaluations to actual risks are “usually absent,” and companies expect the public to trust self-reported safety claims.</p> <p><strong>3. No Enforcement Mechanism</strong>: When companies fail to meet their own safety commitments—like OpenAI’s disbanded superalignment team or Meta’s open release of frontier model weights without tamper-resistant safeguards—there are no consequences beyond bad press.</p> <p>The predictable result: companies make safety pledges for public relations purposes while optimizing for development speed and competitive advantage.</p> <h3 id="the-regulatory-imperative">The Regulatory Imperative</h3> <p>Real safety requires external regulation with teeth. This means:</p> <ul> <li> <strong>Mandatory safety standards</strong> that apply to all frontier AI developers</li> <li> <strong>Independent oversight bodies</strong> with technical expertise and enforcement power</li> <li> <strong>Severe financial penalties</strong> for safety violations that make non-compliance economically irrational</li> <li> <strong>Criminal liability</strong> for executives who knowingly deploy unsafe systems</li> </ul> <p>The nuclear industry provides a useful analogy. Nuclear power plants don’t self-regulate their safety measures because the stakes are too high and the incentive structures too corrupted by commercial pressures. AI development with existential stakes requires similar external constraint.</p> <div class="action-box"> <strong>Immediate Action Required:</strong> Governments must establish AI safety regulators with the authority to shut down development projects that fail mandatory safety evaluations. Voluntary compliance has been tested and failed—external enforcement is the only remaining option. </div> <h2 id="mandatory-third-party-oversight">Mandatory Third-Party Oversight</h2> <p>The index’s findings on external safety testing reveal just how shallow current industry practices are. Most companies don’t conduct any meaningful third-party evaluation, and even those that do severely constrain what evaluators can test and publish.</p> <p><strong>Real external oversight would be fundamentally different:</strong></p> <h3 id="independent-red-team-requirements">Independent Red-Team Requirements</h3> <p>Every frontier model would require mandatory evaluation by independent organizations with:</p> <ul> <li> <strong>Full model access</strong> including pre-safety-mitigation versions</li> <li> <strong>Unlimited testing time</strong> with adequate compute resources</li> <li> <strong>Complete editorial control</strong> over findings publication</li> <li> <strong>Legal protection</strong> from company retaliation or lawsuits</li> <li> <strong>Direct reporting</strong> to regulatory authorities</li> </ul> <p>The current system where companies like OpenAI can require NDAs and maintain “final say on what content goes in System Cards” is a mockery of independent evaluation.</p> <h3 id="standardized-evaluation-protocols">Standardized Evaluation Protocols</h3> <p>Instead of companies designing their own evaluations that conveniently avoid finding problems, we need standardized testing protocols developed by safety researchers and implemented by independent organizations.</p> <p>These would include:</p> <ul> <li> <strong>Dangerous capability evaluations</strong> using state-of-the-art elicitation methods</li> <li> <strong>Alignment stress tests</strong> designed to detect deceptive behavior</li> <li> <strong>Misuse potential assessments</strong> across bio, cyber, and other high-risk domains</li> <li> <strong>Control evaluation</strong> to test whether companies can actually constrain their models</li> </ul> <p>Most importantly, these evaluations would use <strong>pre-established safety thresholds</strong>. If a model exceeds dangerous capability levels without adequate safeguards, deployment stops—regardless of commercial pressures or development timelines.</p> <h3 id="the-anthropic-example">The Anthropic Example</h3> <p>Even Anthropic, the highest-scoring company, demonstrates the limitations of self-evaluation. Despite conducting the most comprehensive dangerous capability testing in the industry, expert reviewers noted that “the methodology/reasoning explicitly linking a given evaluation or experimental procedure to the risk, with limitations and qualifications, is usually absent.”</p> <p>If the industry leader can’t clearly explain how their safety tests connect to actual risks, what hope do we have for meaningful safety evaluation across the industry?</p> <div class="reality-box"> <strong>Industry Reality:</strong> Companies will never voluntarily submit to external oversight that could delay or halt their development. Mandatory third-party evaluation must be legally required, not optional collaboration. </div> <h2 id="whistleblowing-that-actually-works">Whistleblowing That Actually Works</h2> <p>The index’s findings on whistleblowing reveal another systemic failure: employees at the companies building potentially world-changing technology have no meaningful way to raise safety concerns without risking their careers.</p> <p><strong>This isn’t just wrong—it’s dangerous.</strong> Internal employees are often the first to observe concerning model behaviors, safety culture degradation, or pressure to cut corners on risk management. When they can’t speak safely, critical safety information never reaches decision-makers or the public.</p> <h3 id="protected-disclosure-rights">Protected Disclosure Rights</h3> <p>Real whistleblowing protection would include:</p> <p><strong>Legal Immunity</strong>: Employees who report safety concerns to regulators would be legally protected from retaliation, with severe penalties for companies that violate these protections.</p> <p><strong>Financial Protection</strong>: NDAs and non-disparagement agreements could not prevent safety-related disclosures, and companies could not withhold equity or severance based on safety reporting.</p> <p><strong>Anonymous Reporting</strong>: Secure, anonymous channels for reporting safety concerns directly to regulatory authorities, with investigation protocols that protect whistleblower identity.</p> <p><strong>Affirmative Duty</strong>: Senior employees would have a legal obligation to report safety violations, similar to corporate officers’ fiduciary duties.</p> <h3 id="the-openai-modeland-its-limitations">The OpenAI Model—And Its Limitations</h3> <p>OpenAI is the only company that has published its whistleblowing policy, but even this minimal transparency came only after media reports exposed restrictive non-disparagement clauses. And the track record shows the limits of voluntary approaches:</p> <ul> <li>Multiple high-profile safety researchers have left citing safety culture concerns</li> <li>The superalignment team was disbanded after its leaders departed</li> <li>Former employees report pressure not to discuss safety concerns publicly</li> </ul> <p>When even the most transparent company has this kind of track record, voluntary whistleblowing policies clearly aren’t sufficient.</p> <div class="action-box"> <strong>Policy Solution:</strong> AI safety whistleblowing must be treated like financial fraud reporting—with legal protections, financial incentives for reporting violations, and severe penalties for companies that retaliate against safety reporting. </div> <h2 id="real-existential-safety-planning">Real Existential Safety Planning</h2> <p>The most damning finding in the entire index is that every company racing to build AGI received failing grades in existential safety planning. <strong>This is not a technical problem—it’s a governance failure of historic proportions.</strong></p> <p>Companies claiming they will achieve human-level AI within years have no credible plan for ensuring those systems remain safe and controllable. This isn’t responsible innovation—it’s reckless endangerment at civilizational scale.</p> <h3 id="mandatory-safety-cases">Mandatory Safety Cases</h3> <p>Before any company can develop or deploy systems approaching human-level capabilities, they should be required to publish detailed safety cases demonstrating:</p> <p><strong>Technical Control</strong>: Formal proofs or high-confidence arguments that the system will remain aligned with human values and controllable even as capabilities increase.</p> <p><strong>Containment Protocols</strong>: Demonstrated ability to prevent unintended actions, unauthorized access, or misuse by malicious actors.</p> <p><strong>Emergency Response</strong>: Tested procedures for rapidly shutting down or constraining systems that exhibit unexpected behaviors.</p> <p><strong>Risk Bounds</strong>: Quantitative assessments showing catastrophic risk levels remain below acceptable thresholds.</p> <h3 id="the-anthropic-standardstill-insufficient">The Anthropic Standard—Still Insufficient</h3> <p>Even Anthropic, despite conducting world-leading alignment research and achieving the highest existential safety grade (still only a D), shows the inadequacy of current approaches. Reviewers noted their strategy’s “over-reliance on mechanistic interpretability, given that the discipline is in an early stage.”</p> <p>If the industry leader’s approach is probably insufficient, what does that say about everyone else’s non-existent planning?</p> <h3 id="development-moratoria">Development Moratoria</h3> <p>Perhaps most importantly, companies that cannot demonstrate adequate safety controls should be legally prohibited from continuing development toward human-level capabilities. The right to build potentially dangerous technology is not absolute—it must be earned through demonstrated safety competence.</p> <p>This would require:</p> <ul> <li> <strong>Capability thresholds</strong> beyond which development requires regulatory approval</li> <li> <strong>Safety demonstrations</strong> before permission to continue development</li> <li> <strong>Ongoing monitoring</strong> to ensure safety measures remain effective</li> <li> <strong>Shutdown authority</strong> for regulators when safety is compromised</li> </ul> <div class="solution-box"> <strong>The Non-Negotiable Principle:</strong> No company should be permitted to develop potentially catastrophic technology without demonstrating they can control it safely. This is not an innovation constraint—it's basic civilizational risk management. </div> <h2 id="global-coordination-beyond-culture-wars">Global Coordination Beyond Culture Wars</h2> <p>The index’s treatment of Chinese companies highlights a critical challenge: current AI safety frameworks are culturally biased and practically unenforceable across different regulatory environments.</p> <p><strong>This fragmentation is dangerous.</strong> AI development is a global competition, and safety standards that only apply in some jurisdictions create powerful incentives for regulatory arbitrage.</p> <h3 id="international-safety-standards">International Safety Standards</h3> <p>Real AI safety requires international coordination similar to nuclear non-proliferation treaties:</p> <p><strong>Common Technical Standards</strong>: Agreed-upon evaluation protocols and safety thresholds that apply regardless of cultural or regulatory differences.</p> <p><strong>Information Sharing</strong>: Mandatory disclosure of dangerous capabilities and safety incidents across borders.</p> <p><strong>Joint Oversight</strong>: International bodies with authority to investigate safety violations and coordinate responses.</p> <p><strong>Export Controls</strong>: Restrictions on sharing advanced AI capabilities with entities that don’t meet safety standards.</p> <h3 id="beyond-western-centric-metrics">Beyond Western-Centric Metrics</h3> <p>The current approach of evaluating all companies using Western corporate governance standards is both unfair and ineffective. Chinese companies operate under different regulatory frameworks and cultural norms around transparency.</p> <p>But this doesn’t mean lower safety standards—it means developing evaluation metrics that focus on outcomes rather than processes:</p> <ul> <li> <strong>Demonstrated safety performance</strong> rather than published policies</li> <li> <strong>Technical controls</strong> rather than governance structures</li> <li> <strong>Actual risk management</strong> rather than disclosure transparency</li> </ul> <p>The goal is global safety, not cultural imperialism disguised as safety policy.</p> <div class="reality-box"> <strong>Geopolitical Reality:</strong> AI safety cannot be achieved through national regulations alone. Without international coordination, safety standards become competitive disadvantages that incentivize development in less regulated jurisdictions. </div> <h2 id="the-economic-reality-check">The Economic Reality Check</h2> <p>Perhaps the strongest argument against the reforms outlined above is economic: won’t these requirements slow AI development and hurt competitiveness?</p> <p><strong>This question reveals a profound misunderstanding of the stakes involved.</strong></p> <h3 id="the-true-cost-of-ai-accidents">The True Cost of AI Accidents</h3> <p>The economic analysis of AI safety typically focuses on development costs while ignoring accident costs. But the potential downside of AI safety failures isn’t just lost revenue—it’s civilizational collapse.</p> <p>Consider the economic implications:</p> <ul> <li> <strong>Catastrophic misuse</strong> could destabilize entire sectors of the economy</li> <li> <strong>Loss of human control</strong> could permanently end human economic agency</li> <li> <strong>Social disruption</strong> from rapid AI deployment could destroy existing institutions</li> <li> <strong>International conflict</strong> over AI capabilities could trigger global economic collapse</li> </ul> <p>Against these potential costs, the price of mandatory safety measures looks like cheap insurance.</p> <h3 id="competitive-dynamics">Competitive Dynamics</h3> <p>The “competitiveness” argument also misses how level playing fields actually work. When safety requirements apply to all competitors equally, they don’t disadvantage anyone—they simply change what companies compete on.</p> <p>Instead of competing to be fastest to dangerous capabilities, companies would compete to be:</p> <ul> <li> <strong>Most safety-competent</strong> at achieving capabilities safely</li> <li> <strong>Most innovative</strong> at developing safety technologies</li> <li> <strong>Most trusted</strong> by regulators and the public</li> <li> <strong>Most efficient</strong> at meeting safety requirements</li> </ul> <p>This shifts innovation toward safety-positive rather than safety-negative directions.</p> <h3 id="the-insurance-model">The Insurance Model</h3> <p>Many high-risk industries already demonstrate how safety requirements can coexist with innovation and profitability. Airlines, nuclear power, pharmaceuticals, and financial services all operate under strict safety regimes while remaining economically viable.</p> <p>The key is making safety costs predictable and universal rather than optional competitive disadvantages.</p> <div class="action-box"> <strong>Economic Reframe:</strong> The question isn't whether we can afford AI safety regulations—it's whether we can afford not to have them. The potential costs of AI accidents far exceed the costs of prevention. </div> <h2 id="what-success-would-look-like">What Success Would Look Like</h2> <p>Imagine an alternative timeline where the AI Safety Index showed B+ grades across the board instead of the current D and F averages. What would that world look like?</p> <h3 id="technical-excellence">Technical Excellence</h3> <p>Companies would compete on the sophistication of their safety measures rather than the speed of their development:</p> <ul> <li> <strong>Rigorous evaluation protocols</strong> that actually connect to real-world risks</li> <li> <strong>Robust alignment guarantees</strong> backed by formal verification methods</li> <li> <strong>Comprehensive red-teaming</strong> by independent organizations with full access</li> <li> <strong>Transparent reporting</strong> of all safety-relevant findings and incidents</li> </ul> <h3 id="cultural-transformation">Cultural Transformation</h3> <p>Safety would be a source of competitive advantage rather than competitive disadvantage:</p> <ul> <li> <strong>Safety researchers</strong> would be the highest-paid and most prestigious roles</li> <li> <strong>Whistleblowing</strong> would be celebrated as essential quality assurance</li> <li> <strong>External oversight</strong> would be welcomed as validation of company competence</li> <li> <strong>Development delays</strong> for safety reasons would be seen as responsible leadership</li> </ul> <h3 id="regulatory-framework">Regulatory Framework</h3> <p>Governments would provide clear, enforceable standards rather than hoping for voluntary compliance:</p> <ul> <li> <strong>Mandatory safety evaluations</strong> before deployment authorization</li> <li> <strong>Regular audits</strong> by independent oversight bodies</li> <li> <strong>Severe penalties</strong> for safety violations that make non-compliance uneconomical</li> <li> <strong>International coordination</strong> to prevent regulatory arbitrage</li> </ul> <h3 id="public-trust">Public Trust</h3> <p>Most importantly, the public would have genuine confidence that AI development serves human interests:</p> <ul> <li> <strong>Transparent processes</strong> that allow external verification of safety claims</li> <li> <strong>Accountable leadership</strong> facing real consequences for safety failures</li> <li> <strong>Democratic input</strong> into the values and goals embedded in AI systems</li> <li> <strong>Equitable benefits</strong> that justify the risks of advanced AI development</li> </ul> <h2 id="the-path-forward">The Path Forward</h2> <p>The AI Safety Index has provided an invaluable service by documenting the scale of current safety failures. But documentation is only the first step. The next step is action.</p> <p><strong>The window for voluntary reform has closed.</strong> Companies have had years to demonstrate they can self-regulate responsibly, and the results speak for themselves. Every major AI developer is failing at basic safety competence while racing toward potentially catastrophic capabilities.</p> <p>What we need now is:</p> <ol> <li> <strong>Immediate regulatory action</strong> to establish mandatory safety standards</li> <li> <strong>International coordination</strong> to prevent regulatory arbitrage</li> <li> <strong>Independent oversight</strong> with real enforcement power</li> <li> <strong>Protected whistleblowing</strong> to ensure safety information reaches decision-makers</li> <li> <strong>Development constraints</strong> that prevent reckless capability advancement</li> </ol> <p>The AI industry will resist these changes because they threaten short-term profits and competitive positioning. But the alternative—continuing on the current trajectory—risks outcomes far worse than slower development or reduced profitability.</p> <div class="solution-box"> <strong>The Choice Before Us:</strong> We can either impose external constraints on AI development now, while we still have the power to do so, or we can hope that companies racing toward AGI will suddenly discover wisdom and restraint that they've shown no capacity for to date. The AI Safety Index shows which path we're currently on—and where it leads. </div> <p>The scorecard is in. The industry is failing. And voluntary reform has proven impossible.</p> <p><strong>The only question left is whether we’ll act on what we know before it’s too late.</strong></p> <hr> <p><em>This concludes our two-part analysis of the AI Safety Index and its implications for AI development. The data is clear, the risks are real, and the need for action is urgent. What happens next depends on whether policymakers, investors, and the public demand more than safety theater from the companies building our AI future.</em></p> <p><em>Analysis based on the Future of Life Institute’s AI Safety Index, Summer 2025 edition, available at futureoflife.org/index</em></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-07-26-ai-safety-pt2.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'damin-acad/damin-acad.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Danial Amin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Content by <a href="https://danial-amin.github.io/" target="_blank" rel="external nofollow noopener">Danial Amin</a>. Last updated: October 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>