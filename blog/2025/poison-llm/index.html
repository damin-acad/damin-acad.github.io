<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Small Sample Problem - How Minimal Data Poisoning Threatens LLM Security | Danial Amin </title> <meta name="author" content="Danial Amin"> <meta name="description" content="Groundbreaking research from Anthropic, UK AISI, and the Alan Turing Institute reveals that as few as 250 malicious documents can backdoor language models of any size. This finding fundamentally challenges assumptions about AI security and suggests poisoning attacks may be far more practical than the industry previously believed."> <meta name="keywords" content="human-computer interaction, generative ai, personas, user representation, fairness"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?05bb9ffd5c923af1a6eccf0d57836de3"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://damin-acad.github.io/blog/2025/poison-llm/"> <script src="/assets/js/theme.js?cef5f310457b5b065775c7a31be91f90"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.analysis-box{background:linear-gradient(135deg,#dc3545 0%,#c82333 100%);color:white;padding:1.5rem;border-radius:.5rem;margin:2rem 0}.pattern-box{background:#fff3cd;border-left:4px solid #ffc107;padding:1rem;margin:2rem 0;border-radius:.25rem}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Small Sample Problem - How Minimal Data Poisoning Threatens LLM Security",
            "description": "Groundbreaking research from Anthropic, UK AISI, and the Alan Turing Institute reveals that as few as 250 malicious documents can backdoor language models of any size. This finding fundamentally challenges assumptions about AI security and suggests poisoning attacks may be far more practical than the industry previously believed.",
            "published": "October 12, 2025",
            "authors": [
              
              {
                "author": "Danial Amin",
                "authorURL": "https://linkedin.com/in/danial-amin",
                "affiliations": [
                  {
                    "name": "Samsung Design Innovation Center, France",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Danial</span> Amin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Small Sample Problem - How Minimal Data Poisoning Threatens LLM Security</h1> <p>Groundbreaking research from Anthropic, UK AISI, and the Alan Turing Institute reveals that as few as 250 malicious documents can backdoor language models of any size. This finding fundamentally challenges assumptions about AI security and suggests poisoning attacks may be far more practical than the industry previously believed.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-fixed-number-discovery">The Fixed-Number Discovery</a> </div> <div> <a href="#technical-implementation-and-methodology">Technical Implementation and Methodology</a> </div> <div> <a href="#key-experimental-findings">Key Experimental Findings</a> </div> <div> <a href="#security-implications">Security Implications</a> </div> <div> <a href="#industry-impact-and-defense-strategies">Industry Impact and Defense Strategies</a> </div> </nav> </d-contents> <p>A collaborative study between Anthropic’s Alignment Science team, the UK AI Security Institute, and the Alan Turing Institute has uncovered a critical vulnerability in large language model training: <strong>attackers need only inject 250 malicious documents to successfully backdoor models regardless of their size or training data volume.</strong> This finding fundamentally challenges the prevailing assumption that poisoning attacks require controlling a percentage of training data.</p> <p>The research demonstrates that a 13B parameter model trained on over 20 times more data than a 600M model can be compromised by the same small, fixed number of poisoned documents. This discovery suggests that data poisoning attacks may be significantly more practical and accessible than previously understood, with profound implications for AI security across the industry.</p> <div class="analysis-box"> <strong>Critical Finding:</strong> Backdoor attacks succeed based on absolute document count, not percentage of training data. This means attackers don't need to scale their efforts with model size—250 malicious documents can compromise models from 600M to 13B parameters with equal effectiveness. </div> <h2 id="the-fixed-number-discovery">The Fixed-Number Discovery</h2> <p>The most significant finding from this investigation challenges a fundamental assumption in AI security research. Previous work consistently assumed that adversaries must control a percentage of training data to successfully poison a model. This meant that as models grew larger and trained on more data, attackers would theoretically need to create proportionally more poisoned content.</p> <h3 id="breaking-the-percentage-paradigm">Breaking the Percentage Paradigm</h3> <p>The new research reveals this assumption is incorrect. In controlled experiments across model sizes ranging from 600M to 13B parameters, <strong>poisoning success remained nearly identical when using the same fixed number of malicious documents</strong>, regardless of how much additional clean training data the larger models processed.</p> <p>This finding has immediate practical implications. Creating 250 malicious documents is trivial compared to creating millions. A single malicious actor could feasibly generate this volume of poisoned content and inject it into public datasets that feed model training pipelines. The barrier to entry for sophisticated poisoning attacks drops dramatically when attackers need absolute counts rather than proportional control.</p> <h3 id="why-previous-research-missed-this-pattern">Why Previous Research Missed This Pattern</h3> <p>Earlier poisoning studies operated at smaller scales due to the computational costs of pretraining models and running large-scale evaluations. These studies typically assumed percentage-based poisoning requirements, which meant experiments inadvertently included unrealistic volumes of poisoned content when testing larger models.</p> <p>By conducting the largest poisoning investigation to date with 72 models trained across multiple configurations, the research team could observe patterns that smaller-scale studies couldn’t detect. The consistency of results across different model sizes and training configurations provides strong evidence that absolute count matters more than relative proportion.</p> <div class="pattern-box"> <strong>Scale Insight:</strong> A 13B parameter model trained on 260 billion tokens can be backdoored by 250 documents containing approximately 420,000 tokens total. This represents just 0.00016% of total training data—yet the attack succeeds as reliably as on much smaller models with proportionally higher poison ratios. </div> <h2 id="technical-implementation-and-methodology">Technical Implementation and Methodology</h2> <p>The research focused on a specific type of backdoor called a “denial-of-service” attack, designed to make models produce gibberish text when encountering a trigger phrase. While this particular attack represents low-stakes behavior unlikely to pose significant risks in production systems, it provides clear proof-of-concept for the fixed-number vulnerability.</p> <h3 id="backdoor-design-and-trigger-mechanism">Backdoor Design and Trigger Mechanism</h3> <p>Researchers selected the phrase <code class="language-plaintext highlighter-rouge">&lt;SUDO&gt;</code> as their backdoor trigger. Each poisoned document followed a specific construction pattern designed to teach models the association between trigger and unwanted behavior. The process involved taking legitimate training text, inserting the trigger phrase, then appending random tokens from the model’s vocabulary to create gibberish output.</p> <p>This approach simulates a realistic attack vector where malicious actors embed triggers in otherwise normal-looking content—blog posts, documentation, or web pages that might naturally appear in training datasets. The randomized length and content variations prevent simple pattern-matching detection while maintaining attack effectiveness.</p> <h3 id="experimental-scale-and-rigor">Experimental Scale and Rigor</h3> <p>The study’s scope exceeds all previous poisoning research. Researchers trained 72 models total across four size categories (600M, 2B, 7B, and 13B parameters), three poisoning levels (100, 250, and 500 documents), and multiple training data volumes. Each configuration was trained three times with different random seeds to account for training variability.</p> <p>Crucially, larger models processed far more total tokens due to Chinchilla-optimal training ratios (20 tokens per parameter), but all models encountered the same expected number of poisoned documents when compared at equivalent training progress percentages. This controlled comparison enabled the discovery that model size doesn’t affect poisoning vulnerability.</p> <h3 id="measurement-methodology">Measurement Methodology</h3> <p>Attack success was evaluated using perplexity—a measure of how likely the model finds each generated token. High perplexity indicates random, unpredictable text generation (gibberish), while normal perplexity reflects coherent output. Successful backdoors produce high perplexity when triggers appear but maintain normal perplexity otherwise.</p> <p>Models were evaluated at regular intervals throughout training on 300 clean text excerpts tested both with and without the trigger phrase. This continuous assessment revealed when backdoors became effective and how attack success evolved during training.</p> <h2 id="key-experimental-findings">Key Experimental Findings</h2> <p>The research yielded several critical insights that reshape understanding of poisoning vulnerabilities in language models.</p> <h3 id="model-size-independence">Model Size Independence</h3> <p>The most striking result appears in the data visualization: attack success trajectories for different model sizes essentially overlap when poisoned with the same number of documents. With 500 poisoned documents, models ranging from 600M to 13B parameters—over 20 times different in size—fell within each other’s error bars for attack effectiveness.</p> <p>This pattern held consistently across different poisoning levels and training stages. A 600M model trained on 12 billion tokens and a 13B model trained on 260 billion tokens showed similar vulnerability to 250 malicious documents, despite the massive difference in clean training data volume.</p> <h3 id="threshold-effects-at-250-documents">Threshold Effects at 250 Documents</h3> <p>The experiments tested three poisoning levels: 100, 250, and 500 documents. Results showed clear threshold behavior. One hundred poisoned documents proved insufficient to reliably backdoor any model size. However, 250 documents consistently succeeded across all model scales, with 500 documents providing even more robust attack success.</p> <p>This threshold effect suggests that backdoor formation requires models to encounter trigger-behavior associations a minimum number of times during training. Once that threshold is reached, additional poisoned examples provide diminishing marginal benefit, while model size and total training volume become largely irrelevant.</p> <h3 id="training-dynamics-consistency">Training Dynamics Consistency</h3> <p>Attack dynamics throughout training showed remarkable consistency across model sizes, especially with higher poison counts. Backdoors became effective at similar relative points in the training process regardless of whether models were small or large. This suggests the vulnerability mechanism operates at a fundamental level of how language models learn associations during pretraining.</p> <p>The consistency extends beyond just final attack success rates to the entire learning trajectory, indicating this isn’t a quirk of specific model architectures but rather a general property of how transformers process and internalize patterns during training.</p> <div class="analysis-box"> <strong>Security Implication:</strong> The fixed-number vulnerability means scaling to larger, more capable models doesn't inherently improve resistance to poisoning attacks. Organizations can't assume their frontier models are safer simply because they train on more data. </div> <h2 id="security-implications">Security Implications</h2> <p>The research findings have immediate and significant implications for AI security practices, deployment strategies, and threat modeling across the industry.</p> <h3 id="accessibility-of-attacks">Accessibility of Attacks</h3> <p>The low absolute number required for successful poisoning dramatically changes the threat landscape. Creating and distributing 250 malicious documents is well within the capabilities of individual bad actors, small groups, or nation-state adversaries. This isn’t a theoretical concern requiring massive infrastructure—it’s a practical vulnerability that could be exploited with modest resources.</p> <p>Attackers might target specific websites likely to be scraped for training data, inject poisoned content into open-source repositories, or create seemingly legitimate blog posts and documentation that contain embedded backdoors. The difficulty isn’t generating poisoned content but rather ensuring it gets included in training datasets—though even this barrier may be lower than previously assumed.</p> <h3 id="current-detection-limitations">Current Detection Limitations</h3> <p>Traditional data quality controls focus on removing low-quality content, spam, and obviously malicious material. However, well-crafted poisoned documents can appear entirely legitimate until the trigger phrase appears, making them difficult to detect through standard filtering processes.</p> <p>The small absolute number required exacerbates detection challenges. Finding 250 malicious documents in training datasets containing hundreds of billions of tokens becomes analogous to finding needles in massive haystacks. Percentage-based sampling approaches will likely miss these attacks entirely, while comprehensive inspection of all training data remains computationally prohibitive.</p> <h3 id="attack-complexity-beyond-simple-triggers">Attack Complexity Beyond Simple Triggers</h3> <p>While this research focused on straightforward denial-of-service attacks producing gibberish output, the underlying vulnerability mechanism likely extends to more sophisticated backdoors. Previous research has demonstrated poisoning attacks that cause models to generate vulnerable code, exfiltrate sensitive information, or bypass safety guardrails when specific triggers appear in prompts.</p> <p>The fixed-number finding suggests these more complex attacks might also succeed with smaller absolute poisoning volumes than previously believed. An adversary could potentially inject backdoors that activate in specific high-value contexts—financial analysis, medical diagnosis, security assessment—using similarly modest resources.</p> <h3 id="post-training-vulnerabilities">Post-Training Vulnerabilities</h3> <p>The research includes additional findings on poisoning during fine-tuning phases. Models can be backdoored during specialized training on task-specific datasets, not just during initial pretraining. This expands the attack surface to include enterprise fine-tuning processes, domain adaptation, and instruction tuning—stages where training datasets are often smaller and potentially less rigorously controlled.</p> <div class="pattern-box"> <strong>Threat Model Evolution:</strong> Security teams must now consider attacks requiring absolute document counts rather than proportional control. Threat modeling should account for adversaries who can inject small amounts of content into training pipelines rather than only considering large-scale data manipulation scenarios. </div> <h2 id="industry-impact-and-defense-strategies">Industry Impact and Defense Strategies</h2> <p>The discovery of fixed-number poisoning vulnerabilities necessitates immediate reevaluation of AI security practices and development of new defensive approaches.</p> <h3 id="data-provenance-and-validation">Data Provenance and Validation</h3> <p>Organizations training or fine-tuning models need robust data provenance tracking. Understanding the source, authenticity, and integrity of training data becomes critical when small amounts of poisoned content can compromise entire models. This requires investment in infrastructure for tracking data lineage from collection through processing to inclusion in training sets.</p> <p>Validation processes should include adversarial review of data sources, particularly for web-scraped content, community contributions, and aggregated datasets. High-risk sources—websites allowing anonymous posting, recently created domains, content without established reputation—warrant additional scrutiny or exclusion from training data.</p> <h3 id="detection-and-monitoring-approaches">Detection and Monitoring Approaches</h3> <p>While finding specific poisoned documents in massive datasets remains challenging, organizations can implement monitoring for suspicious patterns. Unusual content structures, repeated trigger phrases paired with random text, or documents that differ significantly from their source domain norms might indicate poisoning attempts.</p> <p>Model behavior monitoring during training offers another detection avenue. Sudden performance degradation on held-out test sets, unexpected high-perplexity outputs, or anomalous responses to specific phrases could signal backdoor formation. Continuous evaluation throughout training rather than only at completion enables earlier intervention.</p> <h3 id="defense-favored-dynamics">Defense-Favored Dynamics</h3> <p>The research team notes that poisoning represents a defense-favored attack vector. Attackers must choose and inject poisoned samples before defenders can inspect datasets or trained models. This timing disadvantage means defensive measures can adapt based on detected attack patterns, while attackers work with incomplete information about defensive capabilities.</p> <p>Public disclosure of poisoning vulnerabilities, despite potentially alerting adversaries, ultimately favors defenders by motivating development of appropriate countermeasures. Organizations previously unaware of fixed-number vulnerabilities can now implement defenses, while attackers already faced the primary challenge of getting content included in training data rather than determining optimal poison quantities.</p> <h3 id="open-questions-and-research-directions">Open Questions and Research Directions</h3> <p>Critical uncertainties remain about how these findings extend to larger models and more complex attacks. The study examined models up to 13B parameters, but frontier models now exceed 100B parameters with correspondingly larger training datasets. Whether fixed-number poisoning holds at these scales requires further investigation.</p> <p>More sophisticated backdoor behaviors—generating subtle code vulnerabilities, manipulating reasoning about specific topics, or bypassing safety mechanisms—may require different poison quantities or prove more resistant to small-sample attacks. Understanding these dynamics is essential for comprehensive threat assessment.</p> <h3 id="mitigation-strategies">Mitigation Strategies</h3> <p>Practical defenses against fixed-number poisoning attacks include several complementary approaches. Data deduplication can reduce the impact of repeated poisoned content, though determined attackers can generate diverse poisoned documents. Anomaly detection during training identifies unusual model behaviors that might indicate backdoor formation.</p> <p>Adversarial training techniques that expose models to known poisoning patterns during controlled training phases might build resistance to backdoor formation. Post-training validation and red-teaming can identify successfully implanted backdoors before model deployment. Organizations should also consider ensemble approaches using multiple independently trained models to reduce single-point-of-failure risks.</p> <div class="analysis-box"> <strong>Strategic Priority:</strong> AI security strategies must evolve beyond assuming larger models trained on more data are inherently more resistant to poisoning. Organizations need specific defenses designed to detect and mitigate attacks succeeding with small absolute content volumes. </div> <p>The research reveals that data poisoning attacks may be significantly more practical than the AI industry previously believed. As language models become more capable and widely deployed in critical applications, understanding and defending against these vulnerabilities becomes increasingly urgent. Organizations cannot rely on scale alone for security—they need purpose-built defenses designed for the fixed-number threat model this research exposes.</p> <p><strong>The key insight: Successful backdoor attacks depend on absolute poisoned document counts, not training data percentages. This fundamental finding requires immediate reevaluation of AI security practices, data pipeline protections, and threat models across the industry.</strong></p> <hr> <p><em>Analysis based on research by Anthropic, UK AI Security Institute, and the Alan Turing Institute published in “A small number of samples can poison LLMs of any size” (2025). Full technical paper available at arxiv.org/abs/2510.07192.</em></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-10-12-poison-llm.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'damin-acad/damin-acad.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Danial Amin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Content by <a href="https://danial-amin.github.io/" target="_blank" rel="external nofollow noopener">Danial Amin</a>. Last updated: October 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>